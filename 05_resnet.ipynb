{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Residual Network (ResNet) (He et al., 2016):**](https://arxiv.org/abs/1512.03385) is a widely adopted framework that remains one of the most popular off-the-shelf architectures in computer vision, utilizing residual connections to enable training of very deep networks.\n",
    "\n",
    "![](./imgs/Resnet-Architectures-Right-And-Residual-Block-Top-Left-Bottleneck-Layer-Bottom.ppm)\n",
    "> Image Source: [ResearchGate](https://www.researchgate.net/figure/Resnet-Architectures-Right-And-Residual-Block-Top-Left-Bottleneck-Layer-Bottom_fig1_350524328)\n",
    "\n",
    "ResNet introduces residual connections to combat the vanishing gradient problem in deep neural networks. The key idea is to add shortcut connections that skip one or more layers, allowing the network to learn residuals (i.e., $F(x)+x$) rather than the full transformation. The architecture consists of:\n",
    "\n",
    "1. Stem: An initial convolutional layer followed by batch normalization, ReLU, and max pooling.\n",
    "2. Four Stages: Each stage contains multiple residual blocks, with the first block in stages 2â€“4 typically downsampling the feature maps and increasing the channel count.\n",
    "3. Global Average Pooling: Reduces spatial dimensions to 1x1.\n",
    "4. Fully Connected Layer: Outputs class predictions.\n",
    "\n",
    "The two main block types are:\n",
    "1. BasicBlock: Contains two 3x3 convolutional layers; used in ResNet-18 and ResNet-34.\n",
    "2. BottleneckBlock: Contains three convolutional layers (1x1, 3x3, 1x1); used in deeper models like ResNet-50 to reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\" Basic Residual Block (for ResNet-18, ResNet34) \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First 3x3 conv\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second 3x3 conv\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shorcut connection\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Add residual\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, bottleneck_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 convolution to reduce channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 3x3 convolution with stride for downsampling\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            bottleneck_channels, bottleneck_channels, kernel_size=3, stride=stride, padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "\n",
    "        # 1x1 convolution torestore channels\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set channel configurations based on block type\n",
    "        if block == BasicBlock:\n",
    "            self.channels = [64, 128, 256, 512]\n",
    "        elif block == BottleneckBlock:\n",
    "            self.channels = [256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid block type\")\n",
    "\n",
    "        # Stem\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Four stages\n",
    "        self.layer1 = self.make_stage(block, 64, self.channels[0], layers[0], stride=1)\n",
    "        self.layer2 = self.make_stage(block, self.channels[0], self.channels[1], layers[1], stride=2)\n",
    "        self.layer3 = self.make_stage(block, self.channels[1], self.channels[2], layers[2], stride=2)\n",
    "        self.layer4 = self.make_stage(block, self.channels[2], self.channels[3], layers[3], stride=2)\n",
    "\n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self.channels[3], num_classes)\n",
    "\n",
    "    def make_stage(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        \"\"\"Helper function to create a stage of residual blocks.\"\"\"\n",
    "        layers = []\n",
    "        if block == BasicBlock:\n",
    "            # First block with specified stride\n",
    "            layers.append(block(in_channels, out_channels, stride))\n",
    "            # Subsequent blocks with stride 1\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(block(out_channels, out_channels, 1))\n",
    "        elif block == BottleneckBlock:\n",
    "            bottleneck_channels = out_channels // 4\n",
    "            # First block with specified stride\n",
    "            layers.append(block(in_channels, bottleneck_channels, out_channels, stride))\n",
    "            # Subsequent blocks with stride 1\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(block(out_channels, bottleneck_channels, out_channels, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Stages\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes=10):\n",
    "    \"\"\"ResNet-18: 4 stages with 2 BasicBlocks each.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    \"\"\"ResNet-34: 4 stages with 3, 4, 6, 3 BasicBlocks.\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    \"\"\"ResNet-50: 4 stages with 3, 4, 6, 3 BottleneckBlocks.\"\"\"\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def ResNet101(num_classes=10):\n",
    "    \"\"\"ResNet-101: 4 stages with 3, 4, 23, 3 BottleneckBlocks.\"\"\"\n",
    "    return ResNet(BottleneckBlock, [3, 4, 23, 3], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                     Layer Type              Param #         Output Shape\n",
      "===================================================================================\n",
      "conv1                          Conv2d                     9408    (1, 64, 112, 112)\n",
      "bn1                            BatchNorm2d                 128    (1, 64, 112, 112)\n",
      "relu                           ReLU                          0    (1, 64, 112, 112)\n",
      "maxpool                        MaxPool2d                     0      (1, 64, 56, 56)\n",
      "layer1.0.conv1                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.0.bn1                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.0.relu                  ReLU                          0      (1, 64, 56, 56)\n",
      "layer1.0.conv2                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.0.bn2                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.0.shortcut              Identity                      0      (1, 64, 56, 56)\n",
      "layer1.1.conv1                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.1.bn1                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.1.relu                  ReLU                          0      (1, 64, 56, 56)\n",
      "layer1.1.conv2                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.1.bn2                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.1.shortcut              Identity                      0      (1, 64, 56, 56)\n",
      "layer2.0.conv1                 Conv2d                    73856     (1, 128, 28, 28)\n",
      "layer2.0.bn1                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.0.relu                  ReLU                          0     (1, 128, 28, 28)\n",
      "layer2.0.conv2                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.0.bn2                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.0.shortcut.0            Conv2d                     8320                Error\n",
      "layer2.0.shortcut.1            BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.1.conv1                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.1.bn1                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.1.relu                  ReLU                          0     (1, 128, 28, 28)\n",
      "layer2.1.conv2                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.1.bn2                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.1.shortcut              Identity                      0     (1, 128, 28, 28)\n",
      "layer3.0.conv1                 Conv2d                   295168     (1, 256, 14, 14)\n",
      "layer3.0.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.0.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.0.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.0.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.0.shortcut.0            Conv2d                    33024                Error\n",
      "layer3.0.shortcut.1            BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.1.conv1                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.1.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.1.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.1.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.1.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.1.shortcut              Identity                      0     (1, 256, 14, 14)\n",
      "layer4.0.conv1                 Conv2d                  1180160       (1, 512, 7, 7)\n",
      "layer4.0.bn1                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.0.relu                  ReLU                          0       (1, 512, 7, 7)\n",
      "layer4.0.conv2                 Conv2d                  2359808       (1, 512, 7, 7)\n",
      "layer4.0.bn2                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.0.shortcut.0            Conv2d                   131584                Error\n",
      "layer4.0.shortcut.1            BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.1.conv1                 Conv2d                  2359808       (1, 512, 7, 7)\n",
      "layer4.1.bn1                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.1.relu                  ReLU                          0       (1, 512, 7, 7)\n",
      "layer4.1.conv2                 Conv2d                  2359808       (1, 512, 7, 7)\n",
      "layer4.1.bn2                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.1.shortcut              Identity                      0       (1, 512, 7, 7)\n",
      "avgpool                        AdaptiveAvgPool2d             0       (1, 512, 1, 1)\n",
      "fc                             Linear                     5130                Error\n",
      "===================================================================================\n",
      "Total Parameters                                               11186378\n"
     ]
    }
   ],
   "source": [
    "utils.layer_summary(ResNet18(num_classes=10), (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                     Layer Type              Param #         Output Shape\n",
      "===================================================================================\n",
      "conv1                          Conv2d                     9408    (1, 64, 112, 112)\n",
      "bn1                            BatchNorm2d                 128    (1, 64, 112, 112)\n",
      "relu                           ReLU                          0    (1, 64, 112, 112)\n",
      "maxpool                        MaxPool2d                     0      (1, 64, 56, 56)\n",
      "layer1.0.conv1                 Conv2d                     4160      (1, 64, 56, 56)\n",
      "layer1.0.bn1                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.0.relu                  ReLU                          0      (1, 64, 56, 56)\n",
      "layer1.0.conv2                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.0.bn2                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.0.conv3                 Conv2d                    16640     (1, 256, 56, 56)\n",
      "layer1.0.bn3                   BatchNorm2d                 512     (1, 256, 56, 56)\n",
      "layer1.0.shortcut.0            Conv2d                    16640                Error\n",
      "layer1.0.shortcut.1            BatchNorm2d                 512     (1, 256, 56, 56)\n",
      "layer1.1.conv1                 Conv2d                    16448      (1, 64, 56, 56)\n",
      "layer1.1.bn1                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.1.relu                  ReLU                          0      (1, 64, 56, 56)\n",
      "layer1.1.conv2                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.1.bn2                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.1.conv3                 Conv2d                    16640     (1, 256, 56, 56)\n",
      "layer1.1.bn3                   BatchNorm2d                 512     (1, 256, 56, 56)\n",
      "layer1.1.shortcut              Identity                      0     (1, 256, 56, 56)\n",
      "layer1.2.conv1                 Conv2d                    16448      (1, 64, 56, 56)\n",
      "layer1.2.bn1                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.2.relu                  ReLU                          0      (1, 64, 56, 56)\n",
      "layer1.2.conv2                 Conv2d                    36928      (1, 64, 56, 56)\n",
      "layer1.2.bn2                   BatchNorm2d                 128      (1, 64, 56, 56)\n",
      "layer1.2.conv3                 Conv2d                    16640     (1, 256, 56, 56)\n",
      "layer1.2.bn3                   BatchNorm2d                 512     (1, 256, 56, 56)\n",
      "layer1.2.shortcut              Identity                      0     (1, 256, 56, 56)\n",
      "layer2.0.conv1                 Conv2d                    32896     (1, 128, 56, 56)\n",
      "layer2.0.bn1                   BatchNorm2d                 256     (1, 128, 56, 56)\n",
      "layer2.0.relu                  ReLU                          0     (1, 128, 56, 56)\n",
      "layer2.0.conv2                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.0.bn2                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.0.conv3                 Conv2d                    66048     (1, 512, 28, 28)\n",
      "layer2.0.bn3                   BatchNorm2d                1024     (1, 512, 28, 28)\n",
      "layer2.0.shortcut.0            Conv2d                   131584                Error\n",
      "layer2.0.shortcut.1            BatchNorm2d                1024     (1, 512, 28, 28)\n",
      "layer2.1.conv1                 Conv2d                    65664     (1, 128, 28, 28)\n",
      "layer2.1.bn1                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.1.relu                  ReLU                          0     (1, 128, 28, 28)\n",
      "layer2.1.conv2                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.1.bn2                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.1.conv3                 Conv2d                    66048     (1, 512, 28, 28)\n",
      "layer2.1.bn3                   BatchNorm2d                1024     (1, 512, 28, 28)\n",
      "layer2.1.shortcut              Identity                      0     (1, 512, 28, 28)\n",
      "layer2.2.conv1                 Conv2d                    65664     (1, 128, 28, 28)\n",
      "layer2.2.bn1                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.2.relu                  ReLU                          0     (1, 128, 28, 28)\n",
      "layer2.2.conv2                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.2.bn2                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.2.conv3                 Conv2d                    66048     (1, 512, 28, 28)\n",
      "layer2.2.bn3                   BatchNorm2d                1024     (1, 512, 28, 28)\n",
      "layer2.2.shortcut              Identity                      0     (1, 512, 28, 28)\n",
      "layer2.3.conv1                 Conv2d                    65664     (1, 128, 28, 28)\n",
      "layer2.3.bn1                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.3.relu                  ReLU                          0     (1, 128, 28, 28)\n",
      "layer2.3.conv2                 Conv2d                   147584     (1, 128, 28, 28)\n",
      "layer2.3.bn2                   BatchNorm2d                 256     (1, 128, 28, 28)\n",
      "layer2.3.conv3                 Conv2d                    66048     (1, 512, 28, 28)\n",
      "layer2.3.bn3                   BatchNorm2d                1024     (1, 512, 28, 28)\n",
      "layer2.3.shortcut              Identity                      0     (1, 512, 28, 28)\n",
      "layer3.0.conv1                 Conv2d                   131328     (1, 256, 28, 28)\n",
      "layer3.0.bn1                   BatchNorm2d                 512     (1, 256, 28, 28)\n",
      "layer3.0.relu                  ReLU                          0     (1, 256, 28, 28)\n",
      "layer3.0.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.0.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.0.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.0.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.0.shortcut.0            Conv2d                   525312                Error\n",
      "layer3.0.shortcut.1            BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.1.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.1.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.1.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.1.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.1.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.1.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.1.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.1.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.2.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.2.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.2.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.2.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.2.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.2.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.2.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.2.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.3.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.3.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.3.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.3.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.3.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.3.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.3.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.3.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.4.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.4.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.4.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.4.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.4.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.4.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.4.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.4.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.5.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.5.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.5.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.5.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.5.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.5.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.5.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.5.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.6.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.6.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.6.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.6.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.6.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.6.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.6.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.6.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.7.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.7.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.7.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.7.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.7.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.7.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.7.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.7.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.8.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.8.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.8.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.8.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.8.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.8.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.8.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.8.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.9.conv1                 Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.9.bn1                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.9.relu                  ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.9.conv2                 Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.9.bn2                   BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.9.conv3                 Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.9.bn3                   BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.9.shortcut              Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.10.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.10.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.10.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.10.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.10.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.10.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.10.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.10.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.11.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.11.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.11.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.11.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.11.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.11.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.11.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.11.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.12.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.12.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.12.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.12.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.12.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.12.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.12.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.12.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.13.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.13.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.13.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.13.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.13.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.13.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.13.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.13.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.14.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.14.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.14.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.14.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.14.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.14.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.14.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.14.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.15.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.15.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.15.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.15.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.15.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.15.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.15.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.15.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.16.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.16.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.16.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.16.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.16.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.16.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.16.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.16.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.17.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.17.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.17.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.17.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.17.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.17.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.17.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.17.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.18.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.18.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.18.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.18.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.18.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.18.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.18.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.18.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.19.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.19.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.19.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.19.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.19.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.19.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.19.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.19.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.20.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.20.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.20.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.20.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.20.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.20.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.20.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.20.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.21.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.21.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.21.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.21.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.21.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.21.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.21.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.21.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer3.22.conv1                Conv2d                   262400     (1, 256, 14, 14)\n",
      "layer3.22.bn1                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.22.relu                 ReLU                          0     (1, 256, 14, 14)\n",
      "layer3.22.conv2                Conv2d                   590080     (1, 256, 14, 14)\n",
      "layer3.22.bn2                  BatchNorm2d                 512     (1, 256, 14, 14)\n",
      "layer3.22.conv3                Conv2d                   263168    (1, 1024, 14, 14)\n",
      "layer3.22.bn3                  BatchNorm2d                2048    (1, 1024, 14, 14)\n",
      "layer3.22.shortcut             Identity                      0    (1, 1024, 14, 14)\n",
      "layer4.0.conv1                 Conv2d                   524800     (1, 512, 14, 14)\n",
      "layer4.0.bn1                   BatchNorm2d                1024     (1, 512, 14, 14)\n",
      "layer4.0.relu                  ReLU                          0     (1, 512, 14, 14)\n",
      "layer4.0.conv2                 Conv2d                  2359808       (1, 512, 7, 7)\n",
      "layer4.0.bn2                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.0.conv3                 Conv2d                  1050624      (1, 2048, 7, 7)\n",
      "layer4.0.bn3                   BatchNorm2d                4096      (1, 2048, 7, 7)\n",
      "layer4.0.shortcut.0            Conv2d                  2099200                Error\n",
      "layer4.0.shortcut.1            BatchNorm2d                4096      (1, 2048, 7, 7)\n",
      "layer4.1.conv1                 Conv2d                  1049088       (1, 512, 7, 7)\n",
      "layer4.1.bn1                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.1.relu                  ReLU                          0       (1, 512, 7, 7)\n",
      "layer4.1.conv2                 Conv2d                  2359808       (1, 512, 7, 7)\n",
      "layer4.1.bn2                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.1.conv3                 Conv2d                  1050624      (1, 2048, 7, 7)\n",
      "layer4.1.bn3                   BatchNorm2d                4096      (1, 2048, 7, 7)\n",
      "layer4.1.shortcut              Identity                      0      (1, 2048, 7, 7)\n",
      "layer4.2.conv1                 Conv2d                  1049088       (1, 512, 7, 7)\n",
      "layer4.2.bn1                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.2.relu                  ReLU                          0       (1, 512, 7, 7)\n",
      "layer4.2.conv2                 Conv2d                  2359808       (1, 512, 7, 7)\n",
      "layer4.2.bn2                   BatchNorm2d                1024       (1, 512, 7, 7)\n",
      "layer4.2.conv3                 Conv2d                  1050624      (1, 2048, 7, 7)\n",
      "layer4.2.bn3                   BatchNorm2d                4096      (1, 2048, 7, 7)\n",
      "layer4.2.shortcut              Identity                      0      (1, 2048, 7, 7)\n",
      "avgpool                        AdaptiveAvgPool2d             0      (1, 2048, 1, 1)\n",
      "fc                             Linear                    20490                Error\n",
      "===================================================================================\n",
      "Total Parameters                                               42573258\n"
     ]
    }
   ],
   "source": [
    "utils.layer_summary(ResNet101(num_classes=10), (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.CIFAR10DataLoader(batch_size=64, resize=(224, 224))\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss=1.471568859827793, Test Loss=1.5601825858377347, Test Accuracy=0.4766\n",
      "Epoch 2/10: Train Loss=0.9035725659497863, Test Loss=0.8243136933654737, Test Accuracy=0.713\n",
      "Epoch 3/10: Train Loss=0.6738652632288311, Test Loss=1.0093901016909606, Test Accuracy=0.6734\n",
      "Epoch 4/10: Train Loss=0.537425598582191, Test Loss=0.6318783232360888, Test Accuracy=0.7801\n",
      "Epoch 5/10: Train Loss=0.43620732106516125, Test Loss=0.5323172899757981, Test Accuracy=0.8213\n",
      "Epoch 6/10: Train Loss=0.3490293579047446, Test Loss=0.5121434251214289, Test Accuracy=0.8281\n",
      "Epoch 7/10: Train Loss=0.2705655218866628, Test Loss=0.6442782903552815, Test Accuracy=0.7998\n",
      "Epoch 8/10: Train Loss=0.19743331465060296, Test Loss=0.6032314670693343, Test Accuracy=0.8273\n",
      "Epoch 9/10: Train Loss=0.14161263011834202, Test Loss=0.7539974400761781, Test Accuracy=0.8063\n",
      "Epoch 10/10: Train Loss=0.10415992798055033, Test Loss=0.6644984247388354, Test Accuracy=0.8266\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNet18(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}: Train Loss={train_loss}, Test Loss={test_loss}, Test Accuracy={test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Residual Network (ResNet) (He et al., 2016):**](https://arxiv.org/abs/1512.03385) is a widely adopted framework that remains one of the most popular off-the-shelf architectures in computer vision, utilizing residual connections to enable training of very deep networks.\n",
    "\n",
    "![](./imgs/Resnet-Architectures-Right-And-Residual-Block-Top-Left-Bottleneck-Layer-Bottom.ppm)\n",
    "\n",
    "[ResNet Architecture](https://www.researchgate.net/figure/Resnet-Architectures-Right-And-Residual-Block-Top-Left-Bottleneck-Layer-Bottom_fig1_350524328)\n",
    "\n",
    "ResNet introduces residual connections to combat the vanishing gradient problem in deep neural networks. The key idea is to add shortcut connections that skip one or more layers, allowing the network to learn residuals (i.e., $F(x)+x$) rather than the full transformation. The architecture consists of:\n",
    "\n",
    "1. Stem: An initial convolutional layer followed by batch normalization, ReLU, and max pooling.\n",
    "2. Four Stages: Each stage contains multiple residual blocks, with the first block in stages 2â€“4 typically downsampling the feature maps and increasing the channel count.\n",
    "3. Global Average Pooling: Reduces spatial dimensions to 1x1.\n",
    "4. Fully Connected Layer: Outputs class predictions.\n",
    "\n",
    "The two main block types are:\n",
    "1. BasicBlock: Contains two 3x3 convolutional layers; used in ResNet-18 and ResNet-34.\n",
    "2. BottleneckBlock: Contains three convolutional layers (1x1, 3x3, 1x1); used in deeper models like ResNet-50 to reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\" Basic Residual Block (for ResNet-18, ResNet34) \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First 3x3 conv\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second 3x3 conv\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shorcut connection\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Add residual\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, bottleneck_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 convolution to reduce channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 3x3 convolution with stride for downsampling\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            bottleneck_channels, bottleneck_channels, kernel_size=3, stride=stride, padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "\n",
    "        # 1x1 convolution torestore channels\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set channel configurations based on block type\n",
    "        if block == BasicBlock:\n",
    "            self.channels = [64, 128, 256, 512]\n",
    "        elif block == BottleneckBlock:\n",
    "            self.channels = [256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid block type\")\n",
    "\n",
    "        # Stem\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Four stages\n",
    "        self.layer1 = self.make_stage(block, 64, self.channels[0], layers[0], stride=1)\n",
    "        self.layer2 = self.make_stage(block, self.channels[0], self.channels[1], layers[1], stride=2)\n",
    "        self.layer3 = self.make_stage(block, self.channels[1], self.channels[2], layers[2], stride=2)\n",
    "        self.layer4 = self.make_stage(block, self.channels[2], self.channels[3], layers[3], stride=2)\n",
    "\n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self.channels[3], num_classes)\n",
    "\n",
    "    def make_stage(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        \"\"\"Helper function to create a stage of residual blocks.\"\"\"\n",
    "        layers = []\n",
    "        if block == BasicBlock:\n",
    "            # First block with specified stride\n",
    "            layers.append(block(in_channels, out_channels, stride))\n",
    "            # Subsequent blocks with stride 1\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(block(out_channels, out_channels, 1))\n",
    "        elif block == BottleneckBlock:\n",
    "            bottleneck_channels = out_channels // 4\n",
    "            # First block with specified stride\n",
    "            layers.append(block(in_channels, bottleneck_channels, out_channels, stride))\n",
    "            # Subsequent blocks with stride 1\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(block(out_channels, bottleneck_channels, out_channels, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Stages\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes=10):\n",
    "    \"\"\"ResNet-18: 4 stages with 2 BasicBlocks each.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    \"\"\"ResNet-34: 4 stages with 3, 4, 6, 3 BasicBlocks.\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    \"\"\"ResNet-50: 4 stages with 3, 4, 6, 3 BottleneckBlocks.\"\"\"\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def ResNet101(num_classes=10):\n",
    "    \"\"\"ResNet-101: 4 stages with 3, 4, 23, 3 BottleneckBlocks.\"\"\"\n",
    "    return ResNet(BottleneckBlock, [3, 4, 23, 3], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.CIFAR10DataLoader(batch_size=64, resize=(224, 224))\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Train Loss: 1.3668 | Test Loss: 1.3551 | Test Acc: 0.5600\n",
      "Epoch  2/10 | Train Loss: 0.8166 | Test Loss: 0.7553 | Test Acc: 0.7433\n",
      "Epoch  3/10 | Train Loss: 0.6042 | Test Loss: 0.8134 | Test Acc: 0.7383\n",
      "Epoch  4/10 | Train Loss: 0.4417 | Test Loss: 0.7290 | Test Acc: 0.7597\n",
      "Epoch  5/10 | Train Loss: 0.3285 | Test Loss: 0.8922 | Test Acc: 0.7389\n",
      "Epoch  6/10 | Train Loss: 0.2117 | Test Loss: 1.0311 | Test Acc: 0.7135\n",
      "Epoch  7/10 | Train Loss: 0.1400 | Test Loss: 1.0164 | Test Acc: 0.7444\n",
      "Epoch  8/10 | Train Loss: 0.0945 | Test Loss: 1.1656 | Test Acc: 0.7377\n",
      "Epoch  9/10 | Train Loss: 0.0717 | Test Loss: 1.0215 | Test Acc: 0.7724\n",
      "Epoch 10/10 | Train Loss: 0.0492 | Test Loss: 0.9823 | Test Acc: 0.7933\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNet18(num_classes=10)\n",
    "model.apply(utils.init_kaiming).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1:>{len(str(epochs))}}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**GoogLeNet (Szegedy et al., 2015)**](https://arxiv.org/abs/1409.4842) is an architecture that uses networks with multi-branch convolutions, introducing the Inception module to capture features at multiple scales efficiently.\n",
    "\n",
    "![](./imgs/googlenet.png)\n",
    "\n",
    "![](./imgs/inception_v1.png)\n",
    "\n",
    "GoogLeNet was designed to improve accuracy while keeping computational cost manageable. It won the ILSVRC-2014 competition with state-of-the-art performance, surpassing AlexNet and VGG by using the Inception module, which allows the network to extract multi-scale features efficiently.\n",
    "\n",
    "Unlike traditional CNNs, GoogLeNet processes different receptive fields (1×1, 3×3, 5×5 convolutions, and pooling layers) in parallel within each Inception module, significantly improving feature extraction. The architecture is 22 layers deep (excluding pooling layers) but remains computationally efficient due to 1×1 convolutions, which reduce the number of parameters by acting as \"bottleneck\" layers. Instead of fully connected layers, GoogLeNet uses Global Average Pooling (GAP) to minimize overfitting and reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        \"\"\" Inception module with four parallel branches. \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Branch 1: 1x1 conv\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1x1, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Branch 2: 1x1 conv -> 3x3 conv\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Branch 3: 1x1 conv -> 5x5 conv\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Branch 4: 3x3 max pool -> 1x1 conv\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        \"\"\" Auxiliary classifier used during training to combat vanishing gradients. \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 1024)  # Assumes 14x14 input size after pooling\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = F.relu(self.conv(x), inplace=True)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        \"\"\" GoogLeNet architecture with 9 Inception modules and 2 auxiliary classifiers. \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(True),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # Inception modules (parameters from the original paper)\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)          # Output: 256\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)        # Output: 480\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)         # Output: 512\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)        # Output: 512\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)        # Output: 512\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)        # Output: 528\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)      # Output: 832\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)      # Output: 832\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)      # Output: 1024\n",
    "        \n",
    "        # Auxiliary classifiers\n",
    "        self.aux1 = AuxiliaryClassifier(512, num_classes)  # After inception4a\n",
    "        self.aux2 = AuxiliaryClassifier(528, num_classes)  # After inception4d\n",
    "        \n",
    "        # Main classifier\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        # Initial layers\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        # Inception modules\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.inception4a(x)\n",
    "        if training:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if training:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        # Main classifier\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if training:\n",
    "            return x, aux1, aux2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs one full training pass (epoch) over the training dataset for GoogLeNet.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        model (nn.Module): GoogLeNet model with auxiliary classifiers.\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (Optimizer): Optimizer for updating model weights.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average training loss and accuracy (based on main output).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: GoogLeNet returns (main_output, aux1_output, aux2_output) during training\n",
    "        main_output, aux1_output, aux2_output = model(inputs, training=True)\n",
    "        \n",
    "        # Compute losses for main and auxiliary outputs\n",
    "        loss_main = criterion(main_output, labels)\n",
    "        loss_aux1 = criterion(aux1_output, labels)\n",
    "        loss_aux2 = criterion(aux2_output, labels)\n",
    "        \n",
    "        # Combine losses with weights (as per original GoogLeNet paper: 0.3 for auxiliaries)\n",
    "        loss = loss_main + 0.3 * loss_aux1 + 0.3 * loss_aux2\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy (using main output only for accuracy)\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += (main_output.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_accuracy = running_corrects / len(train_loader.dataset)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.CIFAR10DataLoader(batch_size=64, resize=(224, 224))\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Train Loss: 1.9161 | Test Loss: 1.7627 | Test Acc: 0.3655\n",
      "Epoch  2/10 | Train Loss: 1.4997 | Test Loss: 1.4119 | Test Acc: 0.4806\n",
      "Epoch  3/10 | Train Loss: 1.2306 | Test Loss: 1.2342 | Test Acc: 0.5614\n",
      "Epoch  4/10 | Train Loss: 1.0280 | Test Loss: 0.9727 | Test Acc: 0.6571\n",
      "Epoch  5/10 | Train Loss: 0.8826 | Test Loss: 0.8258 | Test Acc: 0.7087\n",
      "Epoch  6/10 | Train Loss: 0.7561 | Test Loss: 0.7240 | Test Acc: 0.7487\n",
      "Epoch  7/10 | Train Loss: 0.6687 | Test Loss: 0.6983 | Test Acc: 0.7594\n",
      "Epoch  8/10 | Train Loss: 0.5904 | Test Loss: 0.7302 | Test Acc: 0.7471\n",
      "Epoch  9/10 | Train Loss: 0.5287 | Test Loss: 0.5844 | Test Acc: 0.7988\n",
      "Epoch 10/10 | Train Loss: 0.4736 | Test Loss: 0.5735 | Test Acc: 0.7996\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GoogLeNet(num_classes=10)\n",
    "model.apply(utils.init_kaiming).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1:>{len(str(epochs))}}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

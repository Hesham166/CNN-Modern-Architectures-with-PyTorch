{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Network in Network (NiN) (Lin et al., 2013)**](https://arxiv.org/abs/1312.4400) is a pioneering approach that convolves whole neural networks patch-wise over inputs, introducing mlpconv layers to enhance feature abstraction.\n",
    "\n",
    "![](imgs/mlpconv_layer.png)\n",
    "\n",
    "[MLPConv Layer](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "![](imgs/nin.png)\n",
    "\n",
    "[NiN Architecture](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "The Network in Network (NiN) architecture enhances traditional CNNs by incorporating micro-networks within each convolutional layer. Instead of using standard convolutional filters that apply a single linear transformation, NiN replaces them with MLPConv layers, which consist of multiple 1×1 convolutions followed by non-linear activations. This design allows for greater abstraction and feature extraction at each spatial location, making NiN more powerful than traditional CNNs like AlexNet. Additionally, the architecture consists of multiple NiN blocks, where each block includes a standard convolution followed by two 1×1 convolution layers, enabling parameter efficiency while increasing representational capacity.\n",
    "\n",
    "NiN consists of stacked mlpconv blocks, each containing a main convolution followed by two 1x1 convolutions, with max pooling layers interspersed. Here’s the structure:\n",
    "\n",
    "1. Block 1:\n",
    "    - Conv (11x11, 96 filters, stride 4), ReLU\n",
    "    - 1x1 Conv (96 filters), ReLU\n",
    "    - 1x1 Conv (96 filters), ReLU\n",
    "    - MaxPool (3x3, stride 2)\n",
    "2. Block 2:\n",
    "    - Conv (5x5, 256 filters, padding 2), ReLU\n",
    "    - 1x1 Conv (256 filters), ReLU\n",
    "    - 1x1 Conv (256 filters), ReLU\n",
    "    - MaxPool (3x3, stride 2)\n",
    "3. Block 3:\n",
    "    - Conv (3x3, 384 filters, padding 1), ReLU\n",
    "    - 1x1 Conv (384 filters), ReLU\n",
    "    - 1x1 Conv (384 filters), ReLU\n",
    "    - MaxPool (3x3, stride 2)\n",
    "4. Block 4 (Output):\n",
    "    - Conv (3x3, num_classes filters, padding 1), ReLU\n",
    "    - 1x1 Conv (num_classes filters), ReLU\n",
    "    - 1x1 Conv (num_classes filters), ReLU\n",
    "    - Global Average Pooling (reduces spatial dims to 1x1)\n",
    "\n",
    "Input size is 224x224x3 (RGB images). The innovation is the mlpconv block, which acts like a tiny neural network per spatial location, and the use of global average pooling instead of fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlpconv_block(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            mlpconv_block(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "\n",
    "            mlpconv_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "\n",
    "            mlpconv_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "\n",
    "            mlpconv_block(384, num_classes, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3, 224, 224)\n",
      "----------------------------------------\n",
      "Conv2d          output shape: (1, 96, 54, 54)\n",
      "ReLU            output shape: (1, 96, 54, 54)\n",
      "Conv2d          output shape: (1, 96, 54, 54)\n",
      "ReLU            output shape: (1, 96, 54, 54)\n",
      "Conv2d          output shape: (1, 96, 54, 54)\n",
      "ReLU            output shape: (1, 96, 54, 54)\n",
      "MaxPool2d       output shape: (1, 96, 26, 26)\n",
      "Conv2d          output shape: (1, 256, 26, 26)\n",
      "ReLU            output shape: (1, 256, 26, 26)\n",
      "Conv2d          output shape: (1, 256, 26, 26)\n",
      "ReLU            output shape: (1, 256, 26, 26)\n",
      "Conv2d          output shape: (1, 256, 26, 26)\n",
      "ReLU            output shape: (1, 256, 26, 26)\n",
      "MaxPool2d       output shape: (1, 256, 12, 12)\n",
      "Conv2d          output shape: (1, 384, 12, 12)\n",
      "ReLU            output shape: (1, 384, 12, 12)\n",
      "Conv2d          output shape: (1, 384, 12, 12)\n",
      "ReLU            output shape: (1, 384, 12, 12)\n",
      "Conv2d          output shape: (1, 384, 12, 12)\n",
      "ReLU            output shape: (1, 384, 12, 12)\n",
      "MaxPool2d       output shape: (1, 384, 5, 5)\n",
      "Conv2d          output shape: (1, 10, 5, 5)\n",
      "ReLU            output shape: (1, 10, 5, 5)\n",
      "Conv2d          output shape: (1, 10, 5, 5)\n",
      "ReLU            output shape: (1, 10, 5, 5)\n",
      "Conv2d          output shape: (1, 10, 5, 5)\n",
      "ReLU            output shape: (1, 10, 5, 5)\n",
      "AdaptiveAvgPool2d output shape: (1, 10, 1, 1)\n",
      "Flatten         output shape: (1, 10)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "utils.layer_summary(NiN(num_classes=10), (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.CIFAR10DataLoader(batch_size=64, resize=(224, 224))\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Train Loss: 2.1806 | Test Loss: 2.0644 | Test Acc: 0.2335\n",
      "Epoch  2/10 | Train Loss: 1.8938 | Test Loss: 1.7691 | Test Acc: 0.3151\n",
      "Epoch  3/10 | Train Loss: 1.7286 | Test Loss: 1.6825 | Test Acc: 0.3555\n",
      "Epoch  4/10 | Train Loss: 1.6316 | Test Loss: 1.5785 | Test Acc: 0.4017\n",
      "Epoch  5/10 | Train Loss: 1.5501 | Test Loss: 1.4932 | Test Acc: 0.4368\n",
      "Epoch  6/10 | Train Loss: 1.4553 | Test Loss: 1.4203 | Test Acc: 0.4672\n",
      "Epoch  7/10 | Train Loss: 1.3829 | Test Loss: 1.3675 | Test Acc: 0.4898\n",
      "Epoch  8/10 | Train Loss: 1.3296 | Test Loss: 1.3534 | Test Acc: 0.5107\n",
      "Epoch  9/10 | Train Loss: 1.2736 | Test Loss: 1.2767 | Test Acc: 0.5331\n",
      "Epoch 10/10 | Train Loss: 1.2243 | Test Loss: 1.2136 | Test Acc: 0.5578\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NiN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1:>{len(str(epochs))}}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Train Loss: 2.1375 | Test Loss: 2.0493 | Test Acc: 0.2458\n",
      "Epoch  2/10 | Train Loss: 1.9544 | Test Loss: 1.8500 | Test Acc: 0.2598\n",
      "Epoch  3/10 | Train Loss: 1.7363 | Test Loss: 1.5415 | Test Acc: 0.4289\n",
      "Epoch  4/10 | Train Loss: 1.5001 | Test Loss: 1.3435 | Test Acc: 0.5149\n",
      "Epoch  5/10 | Train Loss: 1.2337 | Test Loss: 1.2072 | Test Acc: 0.5745\n",
      "Epoch  6/10 | Train Loss: 1.0496 | Test Loss: 1.1423 | Test Acc: 0.6149\n",
      "Epoch  7/10 | Train Loss: 0.9228 | Test Loss: 0.9431 | Test Acc: 0.6691\n",
      "Epoch  8/10 | Train Loss: 0.8217 | Test Loss: 0.8786 | Test Acc: 0.6922\n",
      "Epoch  9/10 | Train Loss: 0.7569 | Test Loss: 0.9303 | Test Acc: 0.6959\n",
      "Epoch 10/10 | Train Loss: 0.6889 | Test Loss: 0.7740 | Test Acc: 0.7353\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NiN(num_classes=10)\n",
    "model.apply(utils.init_kaiming).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1:>{len(str(epochs))}}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

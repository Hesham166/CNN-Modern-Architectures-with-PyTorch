{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**AlexNet**](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) was the first large-scale network deployed to beat conventional computer vision methods on a large-scale vision challenge, leveraging ReLU activations and dropout for improved performance. \n",
    "\n",
    "![](imgs/AlexNet.png)\n",
    "\n",
    "[AlexNet Architecture](https://neurohive.io/en/popular-networks/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/)\n",
    "\n",
    "AlexNet consists of 8 layers—5 convolutional layers followed by 3 fully connected layers. The network uses ReLU activation, dropout for regularization, and overlapping max-pooling.\n",
    "\n",
    "1. Input Layer\n",
    "   - Takes in a 227×227×3 RGB image (original ImageNet images were 224×224, but AlexNet used a slightly larger input size due to specific kernel strides).\n",
    "   \n",
    "2. First Convolutional Layer (Conv1)\n",
    "   - 96 filters of size 11×11×3, stride 4, ReLU activation\n",
    "   - Output size: 55×55×96\n",
    "   - Followed by Max Pooling (3×3, stride 2) → Output: 27×27×96\n",
    "\n",
    "3. Second Convolutional Layer (Conv2)\n",
    "   - 256 filters of size 5×5×96, stride 1, ReLU activation\n",
    "   - Max Pooling (3×3, stride 2) → Output: 13×13×256\n",
    "\n",
    "4. Third Convolutional Layer (Conv3)\n",
    "   - 384 filters of size 3×3×256, stride 1, ReLU activation\n",
    "   - Output: 13×13×384\n",
    "\n",
    "5. Fourth Convolutional Layer (Conv4)\n",
    "   - 384 filters of size 3×3×384, stride 1, ReLU activation\n",
    "   - Output: 13×13×384\n",
    "\n",
    "6. Fifth Convolutional Layer (Conv5)\n",
    "   - 256 filters of size 3×3×384, stride 1, ReLU activation\n",
    "   - Max Pooling (3×3, stride 2) → Output: 6×6×256\n",
    "\n",
    "7. Fully Connected Layers\n",
    "   - FC6: 4096 neurons, ReLU, Dropout (50%)\n",
    "   - FC7: 4096 neurons, ReLU, Dropout (50%)\n",
    "   - FC8 (Output Layer): 1000 neurons (for ImageNet classes), Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6 * 6 * 256, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3, 227, 227)\n",
      "----------------------------------------\n",
      "Conv2d          output shape: (1, 96, 55, 55)\n",
      "ReLU            output shape: (1, 96, 55, 55)\n",
      "MaxPool2d       output shape: (1, 96, 27, 27)\n",
      "Conv2d          output shape: (1, 256, 27, 27)\n",
      "ReLU            output shape: (1, 256, 27, 27)\n",
      "MaxPool2d       output shape: (1, 256, 13, 13)\n",
      "Conv2d          output shape: (1, 384, 13, 13)\n",
      "ReLU            output shape: (1, 384, 13, 13)\n",
      "Conv2d          output shape: (1, 384, 13, 13)\n",
      "ReLU            output shape: (1, 384, 13, 13)\n",
      "Conv2d          output shape: (1, 256, 13, 13)\n",
      "ReLU            output shape: (1, 256, 13, 13)\n",
      "MaxPool2d       output shape: (1, 256, 6, 6)\n",
      "Flatten         output shape: (1, 9216)\n",
      "Linear          output shape: (1, 4096)\n",
      "ReLU            output shape: (1, 4096)\n",
      "Dropout         output shape: (1, 4096)\n",
      "Linear          output shape: (1, 4096)\n",
      "ReLU            output shape: (1, 4096)\n",
      "Dropout         output shape: (1, 4096)\n",
      "Linear          output shape: (1, 10)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "utils.layer_summary(AlexNet(num_classes=10), (1, 3, 227, 227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.CIFAR10DataLoader(batch_size=64, resize=(227, 227))\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Train Loss: 1.9811 | Test Loss: 1.6664 | Test Acc: 0.3594\n",
      "Epoch  2/10 | Train Loss: 1.4305 | Test Loss: 1.2087 | Test Acc: 0.5628\n",
      "Epoch  3/10 | Train Loss: 1.1476 | Test Loss: 1.0258 | Test Acc: 0.6400\n",
      "Epoch  4/10 | Train Loss: 0.9511 | Test Loss: 0.9292 | Test Acc: 0.6731\n",
      "Epoch  5/10 | Train Loss: 0.8233 | Test Loss: 0.8783 | Test Acc: 0.7014\n",
      "Epoch  6/10 | Train Loss: 0.7192 | Test Loss: 0.7426 | Test Acc: 0.7437\n",
      "Epoch  7/10 | Train Loss: 0.6266 | Test Loss: 0.7846 | Test Acc: 0.7336\n",
      "Epoch  8/10 | Train Loss: 0.5547 | Test Loss: 0.7405 | Test Acc: 0.7560\n",
      "Epoch  9/10 | Train Loss: 0.4914 | Test Loss: 0.6824 | Test Acc: 0.7737\n",
      "Epoch 10/10 | Train Loss: 0.4310 | Test Loss: 0.7219 | Test Acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlexNet(num_classes=10)\n",
    "model.apply(utils.init_kaiming).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1:>{len(str(epochs))}}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

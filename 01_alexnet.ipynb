{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**AlexNet**](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) was the first large-scale network deployed to beat conventional computer vision methods on a large-scale vision challenge, leveraging ReLU activations and dropout for improved performance. \n",
    "\n",
    "![](imgs/AlexNet.png)\n",
    "> Image Source: [Neurohive](https://neurohive.io/en/popular-networks/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/)\n",
    "\n",
    "AlexNet consists of 8 layers—5 convolutional layers followed by 3 fully connected layers. The network uses ReLU activation, dropout for regularization, and overlapping max-pooling.\n",
    "\n",
    "1. **Input Layer**\n",
    "   - Takes in a **227×227×3 RGB** image (original ImageNet images were **224×224**, but AlexNet used a slightly larger input size due to specific kernel strides).\n",
    "   \n",
    "2. **First Convolutional Layer (Conv1)**\n",
    "   - **96 filters** of size **11×11×3**, **stride 4**, **ReLU activation**\n",
    "   - Output size: **55×55×96**\n",
    "   - Followed by **Max Pooling (3×3, stride 2)** → Output: **27×27×96**\n",
    "\n",
    "3. **Second Convolutional Layer (Conv2)**\n",
    "   - **256 filters** of size **5×5×96**, **stride 1**, **ReLU activation**\n",
    "   - **Local Response Normalization (LRN)** (helps generalization)\n",
    "   - **Max Pooling (3×3, stride 2)** → Output: **13×13×256**\n",
    "\n",
    "4. **Third Convolutional Layer (Conv3)**\n",
    "   - **384 filters** of size **3×3×256**, **stride 1**, **ReLU activation**\n",
    "   - Output: **13×13×384**\n",
    "\n",
    "5. **Fourth Convolutional Layer (Conv4)**\n",
    "   - **384 filters** of size **3×3×384**, **stride 1**, **ReLU activation**\n",
    "   - Output: **13×13×384**\n",
    "\n",
    "6. **Fifth Convolutional Layer (Conv5)**\n",
    "   - **256 filters** of size **3×3×384**, **stride 1**, **ReLU activation**\n",
    "   - **Max Pooling (3×3, stride 2)** → Output: **6×6×256**\n",
    "\n",
    "7. **Fully Connected Layers**\n",
    "   - **FC6:** 4096 neurons, ReLU, **Dropout (50%)**\n",
    "   - **FC7:** 4096 neurons, ReLU, **Dropout (50%)**\n",
    "   - **FC8 (Output Layer):** 1000 neurons (for ImageNet classes), **Softmax activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6 * 6 * 256, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Iterate through individual layers in nn.Sequential\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Apply Xavier uniform initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)  # Set biases to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                     Layer Type              Param #         Output Shape\n",
      "===================================================================================\n",
      "net.0                          Conv2d                    34944                Error\n",
      "net.1                          ReLU                          0     (1, 3, 227, 227)\n",
      "net.2                          MaxPool2d                     0     (1, 3, 113, 113)\n",
      "net.3                          Conv2d                   614656                Error\n",
      "net.4                          ReLU                          0     (1, 3, 113, 113)\n",
      "net.5                          MaxPool2d                     0       (1, 3, 56, 56)\n",
      "net.6                          Conv2d                   885120                Error\n",
      "net.7                          ReLU                          0       (1, 3, 56, 56)\n",
      "net.8                          Conv2d                  1327488                Error\n",
      "net.9                          ReLU                          0       (1, 3, 56, 56)\n",
      "net.10                         Conv2d                   884992                Error\n",
      "net.11                         ReLU                          0       (1, 3, 56, 56)\n",
      "net.12                         MaxPool2d                     0       (1, 3, 27, 27)\n",
      "net.13                         Flatten                       0            (1, 2187)\n",
      "net.14                         Linear                 37752832                Error\n",
      "net.15                         ReLU                          0            (1, 2187)\n",
      "net.16                         Dropout                       0            (1, 2187)\n",
      "net.17                         Linear                 16781312                Error\n",
      "net.18                         ReLU                          0            (1, 2187)\n",
      "net.19                         Dropout                       0            (1, 2187)\n",
      "net.20                         Linear                    40970                Error\n",
      "===================================================================================\n",
      "Total Parameters                                               58322314\n"
     ]
    }
   ],
   "source": [
    "utils.layer_summary(AlexNet(num_classes=10), (1, 3, 227, 227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.CIFAR10DataLoader(batch_size=64, resize=(227, 227))\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss=1.7938762808699742, Test Loss=1.421180201184218, Test Accuracy=0.4853\n",
      "Epoch 2/10: Train Loss=1.4135858848729097, Test Loss=1.2837689970708956, Test Accuracy=0.5363\n",
      "Epoch 3/10: Train Loss=1.2614317626294578, Test Loss=1.2204932574253933, Test Accuracy=0.5609\n",
      "Epoch 4/10: Train Loss=1.1539182943456314, Test Loss=1.1173195740219894, Test Accuracy=0.6116\n",
      "Epoch 5/10: Train Loss=1.045770225241361, Test Loss=1.0576315620902237, Test Accuracy=0.62\n",
      "Epoch 6/10: Train Loss=0.9835313361166688, Test Loss=1.0808553107225212, Test Accuracy=0.6205\n",
      "Epoch 7/10: Train Loss=0.9082142593305739, Test Loss=0.9975112623469845, Test Accuracy=0.6487\n",
      "Epoch 8/10: Train Loss=0.841291508558766, Test Loss=1.006145279878264, Test Accuracy=0.6504\n",
      "Epoch 9/10: Train Loss=0.7958463107990792, Test Loss=1.046884818441549, Test Accuracy=0.6398\n",
      "Epoch 10/10: Train Loss=0.7327078013773769, Test Loss=1.040890034216984, Test Accuracy=0.6559\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = utils.train_step(train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = utils.eval_step(test_loader, model, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}: Train Loss={train_loss}, Test Loss={test_loss}, Test Accuracy={test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
